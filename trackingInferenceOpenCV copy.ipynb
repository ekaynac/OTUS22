{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\enes_/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-12-14 Python-3.10.4 torch-1.12.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6144MiB)\n",
      "\n",
      "Loading D:\\Github\\OTUS22\\Weights\\UAVweights300\\best.onnx for ONNX Runtime inference...\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Model weight path\n",
    "MODELPATH = r'D:\\Github\\OTUS22\\Weights\\UAVweights300\\best.onnx'\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom',path=MODELPATH)  # or yolov5n - yolov5x6, custom\n",
    "\n",
    "# Inference\n",
    "model.conf = 0.7 # NMS confidence threshold\n",
    "#model.iou = 0.45  # NMS IoU threshold\n",
    "#model.agnostic = False  # NMS class-agnostic\n",
    "#model.multi_label = False  # NMS multiple labels per box\n",
    "#model.classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\n",
    "model.max_det = 2  # maximum number of detections per image\n",
    "#model.amp = False  # Automatic Mixed Precision (AMP) inference\n",
    "\n",
    "def drawBox(img,bbox):\n",
    "    x,y,w,h = tuple(map(int,bbox))\n",
    "    cv2.rectangle(img,(x,y),((x+w),(y+h)),(255,0,255),3,1)\n",
    "    cv2.putText(img, \"Tracking\", (75, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "def createTracker(num):\n",
    "    tracker = DeepSort(max_age=num)\n",
    "    return tracker\n",
    "\n",
    "# Options for cv2.rectangle\n",
    "thickness=1\n",
    "color=(0,0,255)\n",
    "\n",
    "# Options for cv2.putText\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "org = (50, 50)\n",
    "fontScale = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m object_chips \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mcrop \u001b[39m# your own logic to crop frame based on bbox values\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#embeds = embedder(object_chips) # your own embedder to take in the cropped object chips, and output feature vectors\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m tracks \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39;49mupdate_tracks(bbs, embeds\u001b[39m=\u001b[39;49mobject_chips) \u001b[39m# bbs expected to be a list of detections, each in tuples of ( [left,top,w,h], confidence, detection_class ), also, no need to give frame as your chips has already been embedded\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m track \u001b[39min\u001b[39;00m tracks:\n\u001b[0;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m track\u001b[39m.\u001b[39mis_confirmed():\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\cv\\lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:183\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[1;34m(self, raw_detections, embeds, frame, today, others)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_detections,Iterable)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolygon:\n\u001b[1;32m--> 183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(raw_detections[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m==\u001b[39m\u001b[39m4\u001b[39m\n\u001b[0;32m    184\u001b[0m     raw_detections \u001b[39m=\u001b[39m [d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m raw_detections \u001b[39mif\u001b[39;00m d[\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m d[\u001b[39m0\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Video address\n",
    "VIDPATH = r'F:\\SabitKanatVideolar\\Labellanacakvideolar\\AntalyaFPVteam.mp4'\n",
    "\n",
    "# Video Capturer\n",
    "cap = cv2.VideoCapture(VIDPATH)\n",
    "\n",
    "\n",
    "\n",
    "MAXAGE=5\n",
    "tracker = createTracker(MAXAGE)\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (640,640), interpolation = cv2.INTER_AREA)\n",
    "    if ret:\n",
    "        tracker = DeepSort(max_age=5)\n",
    "        results = model(frame)\n",
    "        bbs =  results.xywh # your own object detection\n",
    "        object_chips = results.crop # your own logic to crop frame based on bbox values\n",
    "        embeds = embedder(object_chips) # your own embedder to take in the cropped object chips, and output feature vectors\n",
    "        tracks = tracker.update_tracks(bbs, embeds=object_chips) # bbs expected to be a list of detections, each in tuples of ( [left,top,w,h], confidence, detection_class ), also, no need to give frame as your chips has already been embedded\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "\n",
    "        cv2.imshow(\"sex\", frame)\n",
    "    else:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(     377.89, dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(results.crop(save=False)[0][\"box\"][0].cpu(), results.crop(save=False)[0][\"box\"][0].cpu(),results.crop(save=False)[0][\"box\"][0].cpu(),results.crop(save=False)[0][\"box\"][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c25497626eb387c829d69d77d2a11d4b3d4e6b3e3b316f5a1837374208b5d84c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
